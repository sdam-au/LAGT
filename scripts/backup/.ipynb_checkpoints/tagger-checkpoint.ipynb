{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pyconll\n",
    "import json\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang=\"el\"\n",
    "# initatize en empty model\n",
    "nlp = spacy.blank(lang)\n",
    "# add the tagger to the pipeline\n",
    "# nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "tagger = nlp.create_pipe(\"tagger\")\n",
    "    # Add the tags. This needs to be done before you start training.\n",
    "#for tag, values in TAG_MAP.items():\n",
    "#    tagger.add_label(tag, values)\n",
    "nlp.add_pipe(tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/ag_lemma_lookup.json\"\n",
    "ag_lemma_lookup = json.load(open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "lemmatizer = Lemmatizer(lookups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our own model with CLI (non-functional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract first part of training data\n",
    "corpus_perseus = pyconll.load.iter_from_url(\"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-Perseus/master/grc_perseus-ud-train.conllu\")\n",
    "\n",
    "corpus_proiel = pyconll.load.iter_from_url(\"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-PROIEL/master/grc_proiel-ud-train.conllu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../depend-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 14.1M  100 14.1M    0     0  12.4M      0  0:00:01  0:00:01 --:--:-- 12.4Mk      0  0:00:04 --:--:--  0:00:04 3260k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2033k  100 2033k    0     0  3656k      0 --:--:-- --:--:-- --:--:-- 3649k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 19.7M  100 19.7M    0     0  15.8M      0  0:00:01  0:00:01 --:--:-- 15.8M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1475k  100 1475k    0     0  2827k      0 --:--:-- --:--:-- --:--:-- 2821k\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-Perseus/master/grc_perseus-ud-train.conllu -o ../depend-data/grc_perseus-ud-train.conllu\n",
    "!curl https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-Perseus/master/grc_perseus-ud-dev.conllu -o ../depend-data/grc_perseus-ud-dev.conllu\n",
    "\n",
    "!curl https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-PROIEL/master/grc_proiel-ud-train.conllu -o ../depend-data/grc_proiel-ud-train.conllu\n",
    "!curl https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-PROIEL/master/grc_proiel-ud-dev.conllu -o ../depend-data/grc_proiel-ud-dev.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grc_proiel-ud-dev.conllu',\n",
       " 'grc_perseus-ud-dev.conllu',\n",
       " 'grc_perseus-ud-train.conllu',\n",
       " 'grc_proiel-ud-train.conllu']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depend_files = os.listdir(\"../depend-data\")\n",
    "depend_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Generated output file (1019 documents):\n",
      "../depend-data/grc_proiel-ud-dev.json\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (1137 documents):\n",
      "../depend-data/grc_perseus-ud-dev.json\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (11476 documents):\n",
      "../depend-data/grc_perseus-ud-train.json\u001b[0m\n",
      "\u001b[38;5;2m✔ Generated output file (15014 documents):\n",
      "../depend-data/grc_proiel-ud-train.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for file in depend_files:\n",
    "    !python3 -m spacy convert ../depend-data/$file ../depend-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../depend-data/grc_perseus-ud-train.json\"\n",
    "grc_perseus_train = json.load(open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pipeline: ['tagger', 'parser', 'ner']\n",
      "Starting with blank model 'xx'\n",
      "Counting training words (limit=0)\n",
      "/home/kasev/.local/lib/python3.6/site-packages/spacy/language.py:639: UserWarning: [W022] Training a new part-of-speech tagger using a model with no lemmatization rules or data. This means that the trained model may not be able to lemmatize correctly. If this is intentional or the language you're using doesn't have lemmatization data, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed.\n",
      "  **kwargs\n",
      "/home/kasev/.local/lib/python3.6/site-packages/spacy/language.py:639: UserWarning: [W033] Training a new part-of-speech tagger using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed. The languages with lexeme normalization tables are currently: da, de, el, en, id, lb, pt, ru, sr, ta, th.\n",
      "  **kwargs\n",
      "/home/kasev/.local/lib/python3.6/site-packages/spacy/language.py:639: UserWarning: [W033] Training a new parser or NER using a model with no lexeme normalization table. This may degrade the performance of the model to some degree. If this is intentional or the language you're using doesn't have a normalization table, please ignore this warning. If this is surprising, make sure you have the spacy-lookups-data package installed. The languages with lexeme normalization tables are currently: da, de, el, en, id, lb, pt, ru, sr, ta, th.\n",
      "  **kwargs\n",
      "\n",
      "Itn  Tag Loss    Tag %    Dep Loss    UAS     LAS    NER Loss   NER P   NER R   NER F   Token %  CPU WPS\n",
      "---  ---------  --------  ---------  ------  ------  ---------  ------  ------  ------  -------  -------\n",
      "  1  82343.693    65.629  204149.670  52.755  42.289      0.000   0.000   0.000   0.000  100.000    10414\n",
      "  2  59012.899    69.288  185324.846  58.031  48.479      0.000   0.000   0.000   0.000  100.000    10970\n",
      "  3  51158.985    70.965  175838.909  59.994  51.017      0.000   0.000   0.000   0.000  100.000    10618\n",
      "  4  46124.976    72.148  168566.800  60.788  52.285      0.000   0.000   0.000   0.000  100.000    10523\n",
      "  5  42396.355    72.830  161740.864  61.792  53.329      0.000   0.000   0.000   0.000  100.000    10571\n",
      "  6  39797.033    73.445  156218.073  61.821  53.545      0.000   0.000   0.000   0.000  100.000    10953\n",
      "  7  37577.909    73.752  151033.759  62.633  54.312      0.000   0.000   0.000   0.000  100.000    10920\n",
      "  8  36256.012    74.145  147149.457  63.255  54.909      0.000   0.000   0.000   0.000  100.000    10910\n",
      "  9  34583.597    74.321  142599.845  63.619  55.499      0.000   0.000   0.000   0.000  100.000    10815\n",
      " 10  33166.698    74.493  138919.401  63.689  55.565      0.000   0.000   0.000   0.000  100.000    10828\n",
      " 11  32085.608    74.570  135682.736  64.177  55.967      0.000   0.000   0.000   0.000  100.000    10788\n",
      " 12  30896.306    74.832  131549.152  63.980  55.846      0.000   0.000   0.000   0.000  100.000    10798\n",
      " 13  30277.056    74.859  130016.904  64.238  56.189      0.000   0.000   0.000   0.000  100.000    10496\n",
      " 14  29369.622    75.030  126678.884  64.362  56.339      0.000   0.000   0.000   0.000  100.000    10034\n",
      " 15  28670.976    75.216  124945.284  64.680  56.490      0.000   0.000   0.000   0.000  100.000     9585\n",
      " 16  27629.714    75.338  122711.262  64.589  56.400      0.000   0.000   0.000   0.000  100.000     9051\n",
      " 17  26976.747    75.401  119951.980  64.650  56.485      0.000   0.000   0.000   0.000  100.000     8552\n",
      " 18  26105.337    75.446  118176.122  64.700  56.561      0.000   0.000   0.000   0.000  100.000     7942\n",
      " 19  25747.222    75.460  116080.060  64.811  56.682      0.000   0.000   0.000   0.000  100.000     6982\n",
      " 20  25157.513    75.546  114155.267  64.801  56.768      0.000   0.000   0.000   0.000  100.000     5832\n",
      " 21  24922.562    75.613  112989.284  64.872  56.935      0.000   0.000   0.000   0.000  100.000     4103\n",
      " 22  24325.410    75.658  111121.878  64.736  56.935      0.000   0.000   0.000   0.000  100.000     2429\n",
      " 23  23642.581    75.667  108507.985  64.821  57.015      0.000   0.000   0.000   0.000  100.000     1339\n",
      " 24  23142.019    75.713  107867.588  64.872  57.101      0.000   0.000   0.000   0.000  100.000      804\n",
      " 25  22788.343    75.835  107163.775  65.028  57.227      0.000   0.000   0.000   0.000  100.000      604\n",
      " 26  22346.801    75.857  105366.028  65.038  57.298      0.000   0.000   0.000   0.000  100.000      568\n",
      " 27  21870.484    75.875  103974.671  65.114  57.414      0.000   0.000   0.000   0.000  100.000      569\n",
      " 28  21722.236    75.880  102576.362  64.948  57.192      0.000   0.000   0.000   0.000  100.000      573\n",
      " 29  21528.114    75.889  102421.955  64.988  57.298      0.000   0.000   0.000   0.000  100.000      570\n",
      " 30  20791.002    75.948  99988.205  64.978  57.328      0.000   0.000   0.000   0.000  100.000      574\n",
      "\u001b[38;5;2m✔ Saved model to output directory\u001b[0m\n",
      "../models/model-final\n",
      "\u001b[2K\u001b[38;5;2m✔ Created best model\u001b[0m\n",
      "../models/model-best\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy train xx ../models ../depend-data/grc_perseus-ud-train.json ../depend-data/grc_perseus-ud-dev.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../models-update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pipeline: ['tagger', 'parser', 'ner']\n",
      "Starting with base model '../models/model-best'\n",
      "Extending component from base model 'tagger'\n",
      "Extending component from base model 'parser'\n",
      "Extending component from base model 'ner'\n",
      "Counting training words (limit=0)\n",
      "\n",
      "Itn  Tag Loss    Tag %    Dep Loss    UAS     LAS    NER Loss   NER P   NER R   NER F   Token %  CPU WPS\n",
      "---  ---------  --------  ---------  ------  ------  ---------  ------  ------  ------  -------  -------\n",
      "  1      0.000     0.000  175985.382  74.553  63.580      0.000   0.000   0.000   0.000  100.000     2616\n",
      "  2      0.000     0.000  161519.461  76.773  65.587      0.000   0.000   0.000   0.000  100.000     2669\n",
      " 13%|████▋                              | 24964/187033 [00:21<02:46, 974.08it/s]^C\n",
      "\u001b[38;5;2m✔ Saved model to output directory\u001b[0m                                  \n",
      "../models-update/model-final\n",
      "\u001b[2K\u001b[38;5;2m✔ Created best model\u001b[0m\n",
      "../models-update/model-best\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/kasev/.local/lib/python3.6/site-packages/spacy/__main__.py\", line 33, in <module>\n",
      "    plac.call(commands[command], sys.argv[1:])\n",
      "  File \"/home/kasev/.local/lib/python3.6/site-packages/plac_core.py\", line 367, in call\n",
      "    cmd, result = parser.consume(arglist)\n",
      "  File \"/home/kasev/.local/lib/python3.6/site-packages/plac_core.py\", line 232, in consume\n",
      "    return cmd, self.func(*(args + varargs + extraopts), **kwargs)\n",
      "  File \"/home/kasev/.local/lib/python3.6/site-packages/spacy/cli/train.py\", line 430, in train\n",
      "    losses=losses,\n",
      "  File \"/home/kasev/.local/lib/python3.6/site-packages/spacy/language.py\", line 529, in update\n",
      "    proc.update(docs, golds, sgd=get_grads, losses=losses, **kwargs)\n",
      "  File \"pipes.pyx\", line 467, in spacy.pipeline.pipes.Tagger.update\n",
      "  File \"/home/kasev/.local/lib/python3.6/site-packages/thinc/neural/_classes/feed_forward.py\", line 46, in begin_update\n",
      "    X, inc_layer_grad = layer.begin_update(X, drop=drop)\n",
      "  File \"/home/kasev/.local/lib/python3.6/site-packages/thinc/api.py\", line 295, in begin_update\n",
      "    X, bp_layer = layer.begin_update(layer.ops.flatten(seqs_in, pad=pad), drop=drop)\n",
      "  File \"/home/kasev/.local/lib/python3.6/site-packages/thinc/neural/_classes/feed_forward.py\", line 46, in begin_update\n",
      "    X, inc_layer_grad = layer.begin_update(X, drop=drop)\n",
      "  File \"/home/kasev/.local/lib/python3.6/site-packages/thinc/neural/_classes/resnet.py\", line 29, in begin_update\n",
      "    y, bp_y = self._layers[0].begin_update(X, drop=drop)\n",
      "  File \"/home/kasev/.local/lib/python3.6/site-packages/thinc/neural/_classes/feed_forward.py\", line 46, in begin_update\n",
      "    X, inc_layer_grad = layer.begin_update(X, drop=drop)\n",
      "  File \"/home/kasev/.local/lib/python3.6/site-packages/thinc/neural/_classes/layernorm.py\", line 62, in begin_update\n",
      "    X, backprop_child = self.child.begin_update(X, drop=0.0)\n",
      "  File \"/home/kasev/.local/lib/python3.6/site-packages/thinc/neural/_classes/maxout.py\", line 76, in begin_update\n",
      "    output__boc = self.ops.gemm(X__bi, W, trans2=True)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy train xx ../models-update ../depend-data/grc_proiel-ud-train.json ../depend-data/grc_proiel-ud-dev.json -b ../models/model-best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training our own model with gold-parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-Perseus/master/grc_perseus-ud-train.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-Perseus/master/grc_perseus-ud-dev.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-PROIEL/master/grc_proiel-ud-train.conllu\",\n",
    "    \"https://raw.githubusercontent.com/UniversalDependencies/UD_Ancient_Greek-PROIEL/master/grc_proiel-ud-dev.conllu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagged_data_from_url(url):\n",
    "    corpus = pyconll.load.iter_from_url(url)\n",
    "    tagged_data = []\n",
    "    for sentence in corpus:\n",
    "        words, tags = [], []\n",
    "        for token in sentence:\n",
    "            words.append(token.form)\n",
    "            tags.append(token.upos)\n",
    "        tagged_data.append((sentence.text, {\"words\" : words, \"tags\" : tags}))\n",
    "    return tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perseus_train = tagged_data_from_url(urls[0])\n",
    "perseus_dev = tagged_data_from_url(urls[1])\n",
    "proiel_train = tagged_data_from_url(urls[2])\n",
    "proiel_dev = tagged_data_from_url(urls[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_perseus = []\n",
    "for sentence in corpus_perseus:\n",
    "    words, tags = [], []\n",
    "    for token in sentence:\n",
    "        words.append(token.form)\n",
    "        tags.append(token.upos)\n",
    "    train_data_perseus.append((sentence.text, {\"words\" : words, \"tags\" : tags}))\n",
    "        #forms_lemmas_dict[token.form] = [{\"l\" : token.lemma, \"p\" : token.xpos, \"s\" : \"\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_proiel = []\n",
    "for sentence in corpus_proiel:\n",
    "    words, tags = [], []\n",
    "    for token in sentence:\n",
    "        words.append(token.form)\n",
    "        tags.append(token.upos)\n",
    "    train_data_proiel.append((sentence.text, {\"words\" : words, \"tags\" : tags}))\n",
    "        #forms_lemmas_dict[token.form] = [{\"l\" : token.lemma, \"p\" : token.xpos, \"s\" : \"\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_proiel[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data_perseus + train_data_proiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang=\"xx\"\n",
    "nlp = spacy.blank(lang)\n",
    "tagger = nlp.create_pipe(\"tagger\")\n",
    "\n",
    "nlp.add_pipe(tagger)\n",
    "\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "n_iter = 10\n",
    "for i in range(n_iter):\n",
    "    random.shuffle(train_data)\n",
    "    losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(texts, annotations, sgd=optimizer, losses=losses)\n",
    "    print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"ἀρχόμενος σέο, Φοῖβε, παλαιγενέων κλέα φωτῶν μνήσομαι, οἳ Πόντοιο κατὰ στόμα καὶ διὰ πέτρας Κυανέας βασιλῆος ἐφημοσύνῃ Πελίαο χρύσειον μετὰ κῶας ἐύζυγον ἤλασαν Ἀργώ.\"\n",
    "doc = nlp(test_text)\n",
    "print(\"Tags\", [(t.text, t.tag_, t.pos_) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"ἀρχόμενος σέο, Φοῖβε, παλαιγενέων κλέα φωτῶν μνήσομαι, οἳ Πόντοιο κατὰ στόμα καὶ διὰ πέτρας Κυανέας βασιλῆος ἐφημοσύνῃ Πελίαο χρύσειον μετὰ κῶας ἐύζυγον ἤλασαν Ἀργώ.\"\n",
    "doc = nlp(test_text)\n",
    "print(\"Tags\", [(t.text, t.tag_, t.pos_) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"../data/spacy_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it back\n",
    "nlp = spacy.load(\"../spacy_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab.lookups.tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab.lookups.remove_table(\"ag_lemma_lookup\")\n",
    "nlp.vocab.lookups.remove_table(\"lemma_exc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/ag_lemma_lookup.json\"\n",
    "lemma_lookup = json.load(open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let add our table to our model\n",
    "#table = nlp.vocab.lookups.add_table(\"ag_lemma_lookup\", ag_lemma_lookup)\n",
    "table = nlp.vocab.lookups.add_table(\"lemma_lookup\", lemma_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"../spacy_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'μιμνήσκω'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_lookup = nlp.vocab.lookups.get_table(\"lemma_lookup\")\n",
    "lemma_lookup[\"VERB\"][\"μνήσομαι\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
